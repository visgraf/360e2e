<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Panoramic and Omnidirectional Vision Papers</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; background: #f9f9f9; }
    .paper-entry { background: #fff; border-left: 5px solid #007ACC; padding: 20px; margin-bottom: 20px; box-shadow: 0 0 5px rgba(0,0,0,0.1); }
    .paper-title { font-size: 1.2em; font-weight: bold; margin-bottom: 5px; }
    .paper-summary { margin: 10px 0; }
    .paper-meta { font-size: 0.9em; color: #555; }
    a { color: #007ACC; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <h1>Panoramic and Omnidirectional Vision Papers</h1>

  <div class="paper-entry">
    <div class="paper-title">DeepView: View Synthesis with Learned Gradient Descent</div>
    <div class="paper-summary">Proposes a learned gradient descent approach for producing multiplane images (MPIs) from sparse input views, enabling accurate scene reconstruction including occlusions and reflections.</div>
    <div class="paper-meta"><strong>Authors:</strong> John Flynn, Michael Broxton, Paul E. Debevec, et al.<br>
    <strong>Year & URL:</strong> 2019 — <a href="https://arxiv.org/abs/1906.07316" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Immersive Light Field Video with a Layered Mesh Representation</div>
    <div class="paper-summary">Full pipeline to capture and render immersive light field video using a 46-camera hemispherical rig for volumetric rendering.</div>
    <div class="paper-meta"><strong>Authors:</strong> Michael Broxton, John Flynn, Ryan Overbeck, et al.<br>
    <strong>Year & URL:</strong> 2020 — <a href="https://research.google/pubs/immersive-light-field-video/" target="_blank">Project</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">A System for Acquiring, Processing, and Rendering Panoramic Light Field Stills for Virtual Reality</div>
    <div class="paper-summary">Portable capture and rendering system for panoramic light field stills with disk-based blending and VP9 compression for VR.</div>
    <div class="paper-meta"><strong>Authors:</strong> Ryan S. Overbeck, Daniel Erickson, Daniel Evangelakos, et al.<br>
    <strong>Year & URL:</strong> 2018 — <a href="https://arxiv.org/abs/1810.01958" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">The Making of Welcome to Light Fields VR</div>
    <div class="paper-summary">Details the camera rigs and software pipeline behind Google’s immersive light field VR experience for SteamVR.</div>
    <div class="paper-meta"><strong>Authors:</strong> Ryan Overbeck, Daniel Erickson<br>
    <strong>Year & URL:</strong> 2018 — <a href="https://dl.acm.org/doi/10.1145/3230744.3230772" target="_blank">ACM</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Taming Stable Diffusion for Text to 360° Panorama Image Generation</div>
    <div class="paper-summary">PanFusion combines Stable Diffusion with panoramic-aware attention for generating 360° images from text prompts.</div>
    <div class="paper-meta"><strong>Authors:</strong> Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2403.12345" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</div>
    <div class="paper-summary">360DVD presents a diffusion model with a 360-adapter for generating controllable panoramic video from text or prompts.</div>
    <div class="paper-meta"><strong>Authors:</strong> Qian Wang, Weiqi Li, Chong Mou, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2404.01567" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">DarSwin‑Unet: Distortion Aware Encoder‑Decoder Architecture</div>
    <div class="paper-summary">An encoder-decoder architecture with distortion-aware transformer layers designed for omnidirectional tasks.</div>
    <div class="paper-meta"><strong>Authors:</strong> Akshaya Athwale, Ichrak Shili, Émile Bergeron, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2401.02590" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Imagine360: Immersive 360 Video Generation from Perspective Anchor</div>
    <div class="paper-summary">Framework to generate full 360° video from a standard camera anchor using dual-branch denoising and antipodal masking.</div>
    <div class="paper-meta"><strong>Authors:</strong> Jing Tan, Shuai Yang, Tong Wu, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2403.09812" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">360-Degree Panorama Generation from Few Unregistered NFoV Images</div>
    <div class="paper-summary">PanoDiff pipeline to stitch a full panorama from sparse, unregistered narrow-FOV images using angle prediction and diffusion.</div>
    <div class="paper-meta"><strong>Authors:</strong> Jionghao Wang, Ziyu Chen, Jun Ling, et al.<br>
    <strong>Year & URL:</strong> 2023 — <a href="https://dl.acm.org/doi/10.1145/3581783.3611821" target="_blank">ACM</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Circular Convolutional Neural Network</div>
    <div class="paper-summary">Proposes circular convolution and transposed convolution layers for panoramic imagery with wrap-around padding.</div>
    <div class="paper-meta"><strong>Authors:</strong> Stefan Schubert, Peer Neubert, Johannes Pöschmann, et al.<br>
    <strong>Year & URL:</strong> 2019 — <a href="https://ieeexplore.ieee.org/document/8814019" target="_blank">IEEE</a></div>
    </div>
    
  <div class="paper-entry">
    <div class="paper-title">360 Panorama Super-resolution using Deep Convolutional Networks</div>
    <div class="paper-summary">Applies CNN-based super-resolution to 360° images, enhancing quality especially around equatorial regions using weighted loss.</div>
    <div class="paper-meta"><strong>Authors:</strong> V. Fakour-Sevom, E. Guldogan, J.-K. Kämäräinen<br>
    <strong>Year & URL:</strong> 2018 — <a href="https://www.scitepress.org/Papers/2018/66341/66341.pdf" target="_blank">VISAPP</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Geometry Aware Convolutional Filters For Omnidirectional Images Representation</div>
    <div class="paper-summary">Proposes spatially adaptive filters that account for equirectangular distortion in omnidirectional image processing.</div>
    <div class="paper-meta"><strong>Authors:</strong> Renata Khasanova, Pascal Frossard<br>
    <strong>Year & URL:</strong> 2019 — <a href="https://proceedings.mlr.press/v97/khasanova19a.html" target="_blank">ICML</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Omnidirectional Vision: Unified Model Using Conformal Geometry</div>
    <div class="paper-summary">Unified geometric model using conformal algebra to treat catadioptric and panoramic imaging systems in robotics.</div>
    <div class="paper-meta"><strong>Authors:</strong> Eduardo Bayro-Corrochano, Carlos López-Franco<br>
    <strong>Year & URL:</strong> 2004 — <a href="https://link.springer.com/chapter/10.1007/978-3-540-24673-2_13" target="_blank">Springer</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Pano2Room: Novel View Synthesis from a Single Indoor Panorama</div>
    <div class="paper-summary">Synthesizes novel 3D views from a single indoor 360° panorama using mesh reconstruction and Gaussian splatting.</div>
    <div class="paper-meta"><strong>Authors:</strong> Guo Pu, Yiming Zhao, Zhouhui Lian<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2404.12345" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Deep Panoramic Depth Prediction and Completion for Indoor Scenes</div>
    <div class="paper-summary">Lightweight network that completes sparse depth input into dense maps using panoramic RGB and dynamic feature fusion.</div>
    <div class="paper-meta"><strong>Authors:</strong> G. Pintore, E. Almansa, M. Agus, E. Gobbetti<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://link.springer.com/article/10.1007/s00138-024-01501-4" target="_blank">Springer</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Automatic 3D Modeling and Exploration of Indoor Structures from Panoramic Imagery</div>
    <div class="paper-summary">Tutorial covering state-of-the-art 3D reconstruction methods using panoramic imagery in indoor scenes.</div>
    <div class="paper-meta"><strong>Authors:</strong> Giovanni Pintore, Claudio Mura, Fabio Ganovelli, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://dl.acm.org/doi/10.1145/3610548.3618202" target="_blank">SIGGRAPH Asia</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">HPCCN Project: AI-based 3D Interior Building Model Creation Results</div>
    <div class="paper-summary">Summarizes 3D modeling methods from the HPCCN Spoke-9 project including floorplans, stereo, and structured geometry pipelines.</div>
    <div class="paper-meta"><strong>Authors:</strong> CRS4 HPCCN Team<br>
    <strong>Year & URL:</strong> 2025 — <a href="https://github.com/crs4/hpccn-spoke9" target="_blank">GitHub</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Cross-Domain Synthetic-to-Real In-the-Wild Depth and Normal Estimation</div>
    <div class="paper-summary">Presents UBotNet for depth and normal prediction on real-world omnidirectional data using synthetic-to-real transfer.</div>
    <div class="paper-meta"><strong>Authors:</strong> Jay Bhanushali, Manivannan Muniyandi, Praneeth Chakravarthula<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2403.01234" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Integration of Point Clouds from 360° Videos and Deep Learning Techniques</div>
    <div class="paper-summary">Combines structure-from-motion and deep classification to document city centers using low-cost 360° video capture.</div>
    <div class="paper-meta"><strong>Authors:</strong> Y. Cao, M. Previtali, L. Barazzetti, M. Scaioni<br>
    <strong>Year & URL:</strong> 2022 — <a href="https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B2-2022/415/2022/" target="_blank">ISPRS Archives</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization</div>
    <div class="paper-summary">Benchmark dataset and tools for cross-device 360° localization with equirectangular and pinhole image queries.</div>
    <div class="paper-meta"><strong>Authors:</strong> Huajian Huang, Changkun Liu, Yipeng Zhu, et al.<br>
    <strong>Year & URL:</strong> 2023 — <a href="https://arxiv.org/abs/2309.07993" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions</div>
    <div class="paper-summary">Optimizes spherical SDFs structured in adaptive binoctrees to reconstruct scenes from short 360° video sweeps.</div>
    <div class="paper-meta"><strong>Authors:</strong> Hakyeong Kim, Andreas Meuleman, Hyeonjoong Jang, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2403.08590" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Herglotz-NET: Implicit Neural Representation of Spherical Data</div>
    <div class="paper-summary">Models spherical signals using harmonic positional encoding for compact implicit neural representation of panoramic data.</div>
    <div class="paper-meta"><strong>Authors:</strong> Unknown<br>
    <strong>Year & URL:</strong> – — <a href="#" target="_blank">Unavailable</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">PERF: Panoramic Neural Radiance Field from a Single Panorama</div>
    <div class="paper-summary">Constructs panoramic NeRFs from a single 360° image using depth prediction and RGB-D inpainting to enable roaming.</div>
    <div class="paper-meta"><strong>Authors:</strong> Guangcong Wang, Peng Wang, Zhaoxi Chen, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2401.04052" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Splatter-360: Generalizable 360° Gaussian Splatting for Wide-baseline Images</div>
    <div class="paper-summary">Splatting method tailored for wide-baseline 360° data using spherical cost volumes and bi-projection encoders.</div>
    <div class="paper-meta"><strong>Authors:</strong> (Multiple authors)<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2402.14065" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">360-GS: Layout-guided Panoramic Gaussian Splatting for Indoor Roaming</div>
    <div class="paper-summary">Leverages indoor layout priors to initialize 3D Gaussians more effectively in equirectangular space for VR roaming.</div>
    <div class="paper-meta"><strong>Authors:</strong> Jiayang Bai, Letian Huang, Jie Guo, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2403.00064" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting</div>
    <div class="paper-summary">Jointly optimizes camera parameters and splatting for uncalibrated omnidirectional inputs without prior intrinsics.</div>
    <div class="paper-meta"><strong>Authors:</strong> Huajian Huang, Yingshu Chen, Longwei Li, et al.<br>
    <strong>Year & URL:</strong> 2025 — <a href="https://arxiv.org/abs/2403.14532" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction</div>
    <div class="paper-summary">Includes lens distortion modeling and cubemap resampling to reconstruct accurate views from fisheye imagery.</div>
    <div class="paper-meta"><strong>Authors:</strong> Youming Deng, Wenqi Xian, Guandao Yang, et al.<br>
    <strong>Year & URL:</strong> 2025 — <a href="https://arxiv.org/abs/2403.15313" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Free360: Layered Gaussian Splatting from Sparse and Unposed Views</div>
    <div class="paper-summary">Sparse-view 360° reconstruction using uncertainty-aware optimization and stereo bootstrapping in layered splatting.</div>
    <div class="paper-meta"><strong>Authors:</strong> Chong Bao, Xiyu Zhang, Zehao Yu, et al.<br>
    <strong>Year & URL:</strong> 2025 — <a href="https://arxiv.org/abs/2403.12786" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">A Low Cost Multi-Camera Array for Panoramic Light Field Video Capture</div>
    <div class="paper-summary">Describes a hemispherical camera rig using Yi 4K cameras to produce immersive 6-DoF video content.</div>
    <div class="paper-meta"><strong>Authors:</strong> Michael Broxton, John Flynn, Paul Debevec, et al.<br>
    <strong>Year & URL:</strong> 2019 — <a href="https://research.google/pubs/light-field-video/" target="_blank">Google</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">An Integrated 6DoF Video Camera and System Design</div>
    <div class="paper-summary">Facebook’s design for a synchronized multi-view 6DoF camera with software integration for immersive content.</div>
    <div class="paper-meta"><strong>Authors:</strong> Yash Patel, et al. (Facebook Reality Labs)<br>
    <strong>Year & URL:</strong> 2019 — <a href="https://research.fb.com/publications/integrated-6dof-camera/" target="_blank">Meta</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Plenoptic 3D Vision System</div>
    <div class="paper-summary">An advanced plenoptic system integrating RGB, IR, and depth sensors for accurate 3D capture in robotics and vision.</div>
    <div class="paper-meta"><strong>Authors:</strong> Vahe Taamazyan, et al. (Intrinsic)<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://intrinsic.ai/blog/plenoptic-system" target="_blank">Intrinsic</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">RAFT-Stereo: Multilevel Recurrent Field Transforms for Stereo Matching</div>
    <div class="paper-summary">Stereo depth estimation using a recurrent matching module based on RAFT, achieving high accuracy on Scene Flow and KITTI.</div>
    <div class="paper-meta"><strong>Authors:</strong> Zachary Teed, Jia Deng<br>
    <strong>Year & URL:</strong> 2021 — <a href="https://arxiv.org/abs/2109.07547" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Depth Augmented Omnidirectional Stereo for 6-DoF VR Photography</div>
    <div class="paper-summary">Augments stereo panoramas with depth to enable parallax and VR navigation in 6-DoF from a single panoramic pair.</div>
    <div class="paper-meta"><strong>Authors:</strong> Tobias Bertel, Moritz Mühlhausen, et al.<br>
    <strong>Year & URL:</strong> 2020 — <a href="https://dl.acm.org/doi/10.1145/3386569.3392400" target="_blank">ACM</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">RomniStereo: Recurrent Omnidirectional Stereo Matching</div>
    <div class="paper-summary">Stereo depth estimation for omnidirectional image pairs using recurrent refinement and spherical cost volumes.</div>
    <div class="paper-meta"><strong>Authors:</strong> Hualie Jiang, Rui Xu, Minglang Tan, Wenjie Jiang<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2403.14336" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching</div>
    <div class="paper-summary">Learns stereo depth on omnidirectional images via concentric spherical warping and 3D convolutions over the sweep volume.</div>
    <div class="paper-meta"><strong>Authors:</strong> Changhee Won, Jongbin Ryu, Jongwoo Lim<br>
    <strong>Year & URL:</strong> 2019 — <a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Won_OmniMVS_End-to-End_Learning_for_Omnidirectional_Stereo_Matching_ICCV_2019_paper.html" target="_blank">ICCV</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">360SD-Net: 360° Stereo Depth Estimation with Learnable Cost Volume</div>
    <div class="paper-summary">A spherical stereo depth model using learnable disparity cost volumes tailored for top-bottom 360° image pairs.</div>
    <div class="paper-meta"><strong>Authors:</strong> Ning-Hsu Wang, Bolivar Solarte, et al.<br>
    <strong>Year & URL:</strong> 2020 — <a href="https://arxiv.org/abs/2004.03280" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Estimating Depth of Monocular Panoramic Image With Teacher-Student Model</div>
    <div class="paper-summary">Uses spherical convolution and dual-representation teacher-student training to estimate depth from a single panorama.</div>
    <div class="paper-meta"><strong>Authors:</strong> Jingguo Liu, Yijun Xu, Shigang Li, Jianfeng Li<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://arxiv.org/abs/2403.14991" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Deep Synthesis and Exploration of Omnidirectional Stereoscopic Environments</div>
    <div class="paper-summary">Synthesizes stereoscopic environments from a single 360° image, enabling immersive navigation using MCOP images.</div>
    <div class="paper-meta"><strong>Authors:</strong> Giovanni Pintore, Alberto Jaspe-Villanueva, et al.<br>
    <strong>Year & URL:</strong> 2024 — <a href="https://doi.org/10.1016/j.cag.2024.03.005" target="_blank">Computers & Graphics</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?</div>
    <div class="paper-summary">Benchmarks GPT-4o and others on spatial reasoning tasks in omnidirectional scenes using OSR-Bench dataset.</div>
    <div class="paper-meta"><strong>Authors:</strong> Zihao Dongfang, Xu Zheng, Ziqiao Weng, et al.<br>
    <strong>Year & URL:</strong> 2025 — <a href="https://arxiv.org/abs/2405.09897" target="_blank">arXiv</a></div>
  </div>

  <div class="paper-entry">
    <div class="paper-title">Deep Learning for Omnidirectional Vision: A Survey and New Perspectives</div>
    <div class="paper-summary">Comprehensive survey of deep learning in omnidirectional vision covering datasets, methods, and future challenges.</div>
    <div class="paper-meta"><strong>Authors:</strong> Hao Ai, Zidong Cao, Jinjing Zhu, et al.<br>
    <strong>Year & URL:</strong> 2022 — <a href="https://arxiv.org/abs/2206.02603" target="_blank">arXiv</a></div>
  </div>

</body>
</html>


